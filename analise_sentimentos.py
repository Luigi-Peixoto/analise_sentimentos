# -*- coding: utf-8 -*-
"""analise_sentimentos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xTLtFpbm22Cc--RvyNPYAgQCMjV42Ld4
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers, optimizers, losses, metrics
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets import imdb
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import string

# Configurações e hiperparâmetros do modelo
h = {
    'max_word_index': 10000,  # Número máximo de palavras a serem consideradas no vocabulário.
                              # Palavras mais raras serão ignoradas durante o treinamento.
    'val_perc': 0.1,  # Porcentagem dos dados a ser utilizada como validação.
    'embedding_dim': 128,  # Dimensão do espaço de embedding (representação vetorial das palavras).
    'batch_size': 32,  # Número de amostras processadas antes de atualizar os pesos do modelo.
    'num_lstm_units': 32,  # Número de unidades LSTM na camada recorrente.
    'lr': 0.001,  # Taxa de aprendizado (learning rate) do otimizador.
    'beta1': 0.9,  # Parâmetro beta1 do otimizador Adam (exponencial para o gradiente).
    'beta2': 0.999,  # Parâmetro beta2 do otimizador Adam (exponencial para o quadrado do gradiente).
    'num_epochs': 10,  # Número de épocas para o treinamento do modelo.
}

# Carregando Base de dados
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=h['max_word_index'])

# Verificar os comprimentos máximo e mínimo das sequências de tokens
# Para entender a variabilidade do tamanho das sequências no dataset.

# Calcular o comprimento máximo das sequências no conjunto de treino
max_seq_len = max([len(seq) for seq in train_data])
print(f'Max sequence length (train): {max_seq_len}')

# Calcular o comprimento máximo das sequências no conjunto de teste
max_seq_len = max([len(seq) for seq in test_data])
print(f'Max sequence length (test): {max_seq_len}')

# Calcular o comprimento mínimo das sequências no conjunto de treino
min_seq_len = min([len(seq) for seq in train_data])
print(f'Min sequence length (train): {min_seq_len}')

# Calcular o comprimento mínimo das sequências no conjunto de teste
min_seq_len = min([len(seq) for seq in test_data])
print(f'Min sequence length (test): {min_seq_len}')

# Gerar um comentário (resenha) e seu rótulo correspondente

# Obter o índice das palavras no dataset IMDB
# `word_index` é um dicionário que mapeia palavras para números inteiros, usado para codificação.
word_index = imdb.get_word_index()

ind = 10

# Criar um dicionário reverso para decodificar os números em palavras
# A chave será o número (índice) e o valor será a palavra correspondente.
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

# Decodificar a resenha
# O dataset IMDB reserva os índices 0, 1 e 2 para tokens especiais.
# Por isso, subtraímos 3 ao índice para acessar a palavra correta no dicionário.
# Se o índice não existir no dicionário, substituímos por '?'.
decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[ind]])


print(decoded_review)
print(train_data[ind])
print(train_labels[ind])

# verify the shape of the training and text matrices
print(train_data.shape)
print(test_data.shape)
print(train_labels.shape)
print(test_labels.shape)

# Ajustar todas as sequências para o mesmo comprimento usando padding (preenchimento)
# Isso garante que o modelo possa processar entradas de tamanhos uniformes.
X_train = keras.preprocessing.sequence.pad_sequences(train_data, maxlen=2494)
X_test = keras.preprocessing.sequence.pad_sequences(test_data, maxlen=2494)

# Converter os rótulos (labels) para o tipo float32
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

# Dividir o conjunto de teste em teste e validação
X_val, X_test_final, y_val, y_test_final = train_test_split(X_test,
                                                            y_test,
                                                            test_size=0.5,
                                                            stratify=y_test)

# Construção do modelo sequencial
model = keras.Sequential([

    # Camada de embedding
    layers.Embedding(input_dim=h['max_word_index'], output_dim=h['embedding_dim']),

     # Primeira camada convolucional
    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),
    layers.MaxPooling1D(pool_size=4), # Reduz a dimensionalidade da saída com uma janela de pooling de tamanho 4.

    # Segunda camada convolucional
    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),
    layers.MaxPooling1D(pool_size=4),

    # Camada de Flatten
    layers.Flatten(),

    # Primeira camada densa
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5), # Regularização para evitar overfitting (desativa 50% dos neurônios aleatoriamente).

    # Camada de saída
    layers.Dense(1, activation='sigmoid')  # Saída para classificação binária (sentimento positivo/negativo)
])
print(model.summary())

# Definição do otimizador Adam
opt = optimizers.Adam(learning_rate=h['lr'],
                      beta_1=h['beta1'],
                      beta_2=h['beta2'])

# model compilation
model.compile(optimizer=opt,
              loss=losses.binary_crossentropy,
              metrics=[metrics.binary_accuracy]) # Métrica para avaliar a precisão em tarefas binárias.

# callbacks
callback_list = [
    # Callback para salvar o melhor modelo baseado na perda de validação
    ModelCheckpoint('model.keras',  # Nome do arquivo para salvar o modelo.
                             monitor='val_loss', # Métrica monitorada para decidir quando salvar.
                             save_best_only=True,
                             mode='min',
                             verbose=1),
    # Callback para interromper o treinamento cedo se o modelo parar de melhorar
    EarlyStopping(monitor='val_loss', # Métrica monitorada para decidir quando parar.
                  patience=3, # Número de épocas sem melhoria antes de interromper o treinamento.
                  restore_best_weights=True)
]

# Treinamento do modelo
history = model.fit(X_train,
                    y_train,
                    epochs=h['num_epochs'],
                    batch_size=h['batch_size'],
                    validation_data=(X_val, y_val),
                    callbacks=callback_list)

# inspect training plots
history_dict = history.history

loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

acc_values = history_dict['binary_accuracy']
val_acc_values = history_dict['val_binary_accuracy']

epochs = range(1, len(loss_values) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

ax1.plot(epochs, loss_values, 'bo', label='Training loss')
ax1.plot(epochs, val_loss_values, 'r', label='Validation loss')
ax1.set_title('Training and validation loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.legend()

ax2.plot(epochs, acc_values, 'bo', label='Training acc')
ax2.plot(epochs, val_acc_values, 'r', label='Validation acc')
ax2.set_title('Training and validation accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.legend()

plt.show()

# Teste do modelo
y_pred = model.predict(X_test_final)
y_pred = np.where(y_pred > 0.5, 1, 0)

# produce confusion matrix
mat = confusion_matrix(y_test_final, y_pred)
mat

# plot confusion matrix
fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax = sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
ax.set_xlabel('true label')
ax.set_ylabel('predicted label')
plt.show()

"""**DEPLOY**"""

# Pré-processa o texto inserido
def preprocess_text(text):
    text = text.lower() # Converte o texto para minúsculas
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove pontuações
    return text.split() # Divide o texto em uma lista de palavras.

# Codifica o texto
def encode_text(text, word_index):
    # Para cada palavra no texto pré-processado, retorna o índice correspondente
    return [word_index.get(word, word_index["<UNK>"]) for word in preprocess_text(text)]

# Decodifica uma sequência de índices numéricos para texto
def decode_text(indices, reverse_word_index):
    # Converte cada índice em uma palavra usando o dicionário reverso
    # Substitui índices desconhecidos por "?"
    return ' '.join([reverse_word_index.get(i, '?') for i in indices])

# Classifica a entrada
def predict(text, best_model):
  sequence = keras.preprocessing.sequence.pad_sequences([text_coded],maxlen=2494)
  prediction = best_model.predict(sequence)

  print(prediction)

  # Verifica se o comentário é positivo ou negativo
  if prediction > 0.5:
      print("Esse comentário é positivo")
  else:
      print("Esse comentário é negativo")

# Carregar o modelo salvo
best_model = load_model("model.keras")

# Obter o índice de palavras
word_index = imdb.get_word_index()

# Adicionar um deslocamento para palavras conhecidas
word_index = {k: (v + 3) for k, v in word_index.items()}

# Definir índices reservados
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2  # Unknown words
word_index["<UNUSED>"] = 3

# Dicionário Reverso
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

# Texto de Input
new_text = input("Digite uma avaliação para análise de sentimento: ")

# Codificar e decodificar o texto
text_coded = encode_text(new_text, word_index)
print("Texto codificado:", text_coded)

decoded_review = decode_text(text_coded, reverse_word_index)
print("Texto decodificado:", decoded_review)

predict(text_coded, best_model)